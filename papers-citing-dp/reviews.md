## Reviews
 18 pieces of Reviews in total. (Edited at 2021-08-09)

@article{behlerFourGenerationsHighdimensional2021,
    abstract = {Since their introduction about 25 years ago, machine learning (ML) potentials have become an important tool in the field of atomistic simulations. After the initial decade, in which neural networks were successfully used to construct potentials for rather small molecular systems, the development of high-dimensional neural network potentials (HDNNPs) in 2007 opened the way for the application of ML potentials in simulations of large systems containing thousands of atoms. To date, many other types of ML potentials have been proposed continuously increasing the range of problems that can be studied. In this review, the methodology of the family of HDNNPs including new recent developments will be discussed using a classification scheme into four generations of potentials, which is also applicable to many other types of ML potentials. The first generation is formed by early neural network potentials designed for low-dimensional systems. High-dimensional neural network potentials established the second generation and are based on three key steps: first, the expression of the total energy as a sum of environment-dependent atomic energy contributions; second, the description of the atomic environments by atom-centered symmetry functions as descriptors fulfilling the requirements of rotational, translational, and permutation invariance; and third, the iterative construction of the reference electronic structure data sets by active learning. In third-generation HDNNPs, in addition, long-range interactions are included employing environment-dependent partial charges expressed by atomic neural networks. In fourth-generation HDNNPs, which are just emerging, in addition, nonlocal phenomena such as long-range charge transfer can be included. The applicability and remaining limitations of HDNNPs are discussed along with an outlook at possible future developments.},
    author = {Behler, Jörg},
    date = {2021},
    doi = {10.1021/acs.chemrev.0c00868},
    journaltitle = {Chemical Reviews},
    publisher = {{American Chemical Society}},
    title = {Four Generations of High-Dimensional Neural Network Potentials},
    url = {https://pubs.acs.org/doi/abs/10.1021/acs.chemrev.0c00868}
}

@article{carleoMachineLearningPhysical2019,
    abstract = {Machine learning (ML) encompasses a broad range of algorithms and modeling tools used for a vast array of data processing tasks, which has entered most scientific disciplines in recent years. This article reviews in a selective way the recent research on the interface between machine learning and the physical sciences. This includes conceptual developments in ML motivated by physical insights, applications of machine learning techniques to several domains in physics, and cross fertilization between the two fields. After giving a basic notion of machine learning methods and principles, examples are described of how statistical physics is used to understand methods in ML. This review then describes applications of ML methods in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. Research and development into novel computing architectures aimed at accelerating ML are also highlighted. Each of the sections describe recent successes as well as domain-specific methodology and challenges.},
    annotation = {WOS:000505697300001},
    author = {Carleo, Giuseppe and Cirac, Ignacio and Cranmer, Kyle and Daudet, Laurent and Schuld, Maria and Tishby, Naftali and Vogt-Maranto, Leslie and Zdeborova, Lenka},
    date = {2019-12-06},
    doi = {10.1103/RevModPhys.91.045002},
    issn = {0034-6861},
    journaltitle = {Reviews of Modern Physics},
    langid = {english},
    location = {{College Pk}},
    number = {4},
    pages = {045002},
    publisher = {{Amer Physical Soc}},
    shortjournal = {Rev. Mod. Phys.},
    title = {Machine Learning and the Physical Sciences},
    url = {https://www.webofscience.com/wos/alldb/summary/44a3404b-9775-4cfb-8e8f-3f1fab40830f-035e7f8d/date-descending/5},
    urldate = {2021-08-06},
    volume = {91}
}

@article{dralMolecularExcitedStates2021,
    abstract = {Theoretical simulations of electronic excitations and associated processes in molecules are indispensable for fundamental research and technological innovations. However, such simulations are notoriously challenging to perform with quantum mechanical methods. Advances in machine learning open many new avenues for assisting molecular excited-state simulations. In this Review, we track such progress, assess the current state of the art and highlight the critical issues to solve in the future. We overview a broad range of machine learning applications in excited-state research, which include the prediction of molecular properties, improvements of quantum mechanical methods for the calculations of excited-state properties and the search for new materials. Machine learning approaches can help us understand hidden factors that influence photo-processes, leading to a better control of such processes and new rules for the design of materials for optoelectronic applications.},
    annotation = {WOS:000652426900001},
    author = {Dral, Pavlo O. and Barbatti, Mario},
    date = {2021-06},
    doi = {10.1038/s41570-021-00278-1},
    journaltitle = {Nature Reviews Chemistry},
    langid = {english},
    location = {{Berlin}},
    number = {6},
    pages = {388--405},
    publisher = {{Nature Research}},
    shortjournal = {Nat. Rev. Chem.},
    title = {Molecular Excited States through a Machine Learning Lens},
    url = {https://www.webofscience.com/wos/alldb/summary/44a3404b-9775-4cfb-8e8f-3f1fab40830f-035e7f8d/date-descending/1},
    urldate = {2021-08-06},
    volume = {5}
}

@article{glielmoUnsupervisedLearningMethods2021,
    abstract = {Unsupervised learning is becoming an essential tool to analyze the increasingly large amounts of data produced by atomistic and molecular simulations, in material science, solid state physics, biophysics, and biochemistry. In this Review, we provide a comprehensive overview of the methods of unsupervised learning that have been most commonly used to investigate simulation data and indicate likely directions for further developments in the field. In particular, we discuss feature representation of molecular systems and present state-of-the-art algorithms of dimensionality reduction, density estimation, and clustering, and kinetic models. We divide our discussion into self-contained sections, each discussing a specific method. In each section, we briefly touch upon the mathematical and algorithmic foundations of the method, highlight its strengths and limitations, and describe the specific ways in which it has been used-or can be used-to analyze molecular simulation data.},
    author = {Glielmo, Aldo and Husic, Brooke E. and Rodriguez, Alex and Clementi, Cecilia and Noé, Frank and Laio, Alessandro},
    date = {2021},
    doi = {10.1021/acs.chemrev.0c01195},
    journaltitle = {Chemical Reviews},
    publisher = {{ACS Publications}},
    title = {Unsupervised {{Learning Methods}} for {{Molecular Simulation Data}}},
    url = {https://pubs.acs.org/doi/abs/10.1021/acs.chemrev.0c01195}
}

@article{greenstreetMachinelearningassistedModeling2021,
    abstract = {By integrating artificial intelligence algorithms and physics-based simulations, researchers are developing new models that are both reliable and interpretable.},
    annotation = {WOS:000668845000013},
    author = {Greenstreet, Sarah},
    date = {2021-07-01},
    doi = {10.1063/PT.3.4794},
    issn = {0031-9228},
    journaltitle = {Physics Today},
    langid = {english},
    location = {{Melville}},
    number = {7},
    pages = {42--47},
    publisher = {{Amer Inst Physics}},
    shortjournal = {Phys. Today},
    title = {Machine-Learning-Assisted Modeling},
    url = {https://www.webofscience.com/wos/alldb/summary/44a3404b-9775-4cfb-8e8f-3f1fab40830f-035e7f8d/date-descending/1},
    urldate = {2021-08-06},
    volume = {74}
}

@article{hartMachineLearningAlloys,
    abstract = {Alloy modelling has a history of machine-learning-like approaches, preceding the tide of data-science-inspired work. The dawn of computational databases has made the integration of analysis, prediction and discovery the key theme in accelerated alloy research. Advances in machine-learning methods and enhanced data generation have created a fertile ground for computational materials science. Pairing machine learning and alloys has proven to be particularly instrumental in pushing progress in a wide variety of materials, including metallic glasses, high-entropy alloys, shape-memory alloys, magnets, superalloys, catalysts and structural materials. This Review examines the present state of machine-learning-driven alloy research, discusses the approaches and applications in the field and summarizes theoretical predictions and experimental validations. We foresee that the partnership between machine learning and alloys will lead to the design of new and improved systems. Machine learning is enabling a metallurgical renaissance. This Review discusses recent progress in representations, descriptors and interatomic potentials, overviewing metallic glasses, high-entropy alloys, superalloys and shape-memory alloys, magnets and catalysts, and the prediction of mechanical and thermal properties.},
    annotation = {WOS:000675035000001},
    author = {Hart, Gus L. W. and Mueller, Tim and Toher, Cormac and Curtarolo, Stefano},
    doi = {10.1038/s41578-021-00340-w},
    issn = {2058-8437},
    journaltitle = {Nature Reviews Materials},
    langid = {english},
    location = {{Berlin}},
    publisher = {{Nature Research}},
    shortjournal = {Nat. Rev. Mater.},
    title = {Machine Learning for Alloys},
    url = {https://www.webofscience.com/wos/alldb/summary/44a3404b-9775-4cfb-8e8f-3f1fab40830f-035e7f8d/date-descending/1},
    urldate = {2021-08-06}
}

@article{karniadakisPhysicsinformedMachineLearning2021,
    abstract = {The rapidly developing field of physics-informed learning integrates data and mathematical models seamlessly, enabling accurate inference of realistic and high-dimensional multiphysics problems. This Review discusses the methodology and provides diverse examples and an outlook for further developments. Despite great progress in simulating multiphysics problems using the numerical discretization of partial differential equations (PDEs), one still cannot seamlessly incorporate noisy data into existing algorithms, mesh generation remains complex, and high-dimensional problems governed by parameterized PDEs cannot be tackled. Moreover, solving inverse problems with hidden physics is often prohibitively expensive and requires different formulations and elaborate computer codes. Machine learning has emerged as a promising alternative, but training deep neural networks requires big data, not always available for scientific problems. Instead, such networks can be trained from additional information obtained by enforcing the physical laws (for example, at random points in the continuous space-time domain). Such physics-informed learning integrates (noisy) data and mathematical models, and implements them through neural networks or other kernel-based regression networks. Moreover, it may be possible to design specialized network architectures that automatically satisfy some of the physical invariants for better accuracy, faster training and improved generalization. Here, we review some of the prevailing trends in embedding physics into machine learning, present some of the current capabilities and limitations and discuss diverse applications of physics-informed learning both for forward and inverse problems, including discovering hidden physics and tackling high-dimensional problems.},
    annotation = {WOS:000653612800001},
    author = {Karniadakis, George Em and Kevrekidis, Ioannis G. and Lu, Lu and Perdikaris, Paris and Wang, Sifan and Yang, Liu},
    date = {2021-06},
    doi = {10.1038/s42254-021-00314-5},
    journaltitle = {Nature Reviews Physics},
    langid = {english},
    location = {{London}},
    number = {6},
    pages = {422--440},
    publisher = {{Springernature}},
    shortjournal = {Nat. Rev. Phys.},
    title = {Physics-Informed Machine Learning},
    url = {https://www.webofscience.com/wos/alldb/summary/44a3404b-9775-4cfb-8e8f-3f1fab40830f-035e7f8d/date-descending/1},
    urldate = {2021-08-06},
    volume = {3}
}

@article{Kocer2021,
    abstract = {In the past two decades, machine learning potentials (MLP) have reached a level of maturity that now enables applications to large-scale atomistic simulations of a wide range of systems in chemistry, physics and materials science. Different machine learning algorithms have been used with great success in the construction of these MLPs. In this review, we discuss an important group of MLPs relying on artificial neural networks to establish a mapping from the atomic structure to the potential energy. In spite of this common feature, there are important conceptual differences, which concern the dimensionality of the systems, the inclusion of long-range electrostatic interactions, global phenomena like non-local charge transfer, and the type of descriptor used to represent the atomic structure, which can either be predefined or learnable. A concise overview is given along with a discussion of the open challenges in the field.},
    author = {Kocer, Emir and Ko, TW Tsz Wai and Behler, Jörg and Behler, J},
    date = {2021-07},
    journaltitle = {arxiv.org},
    title = {Neural Network Potentials: {{A}} Concise Overview of Methods},
    url = {https://arxiv.org/abs/2107.03727 http://arxiv.org/abs/2107.03727}
}

@incollection{komeijiFMOInterfacedMolecular2021,
    abstract = {Three ways to combine FMO and MD are described: FMO-MD, FMO-QM/MM-MD, and MM-MD/FMO. FMO-MD is an ab initio MD in which force is updated on-the-fly by FMO. FMO-QM/MM-MD is a QM/MM-MD method in which the QM part is calculated by FMO. MM-MD/FMO is a simulation protocol in which FMO calculation is performed for molecular configurations generated by MM-MD. The methodology and application of these methods are described and compared.},
    author = {Komeiji, Yuto and Ishikawa, Takeshi},
    booktitle = {Recent {{Advances}} of the {{Fragment Molecular Orbital Method}}},
    date = {2021},
    pages = {373--389},
    publisher = {{Springer}},
    title = {{{FMO Interfaced}} with {{Molecular Dynamics Simulation}}},
    url = {https://link.springer.com/chapter/10.1007/978-981-15-9235-5_19}
}

@article{meuwlyMachineLearningChemical2021,
    abstract = {Machine learning (ML) techniques applied to chemical reactions have a long history. The present contribution discusses applications ranging from small molecule reaction dynamics to computational platforms for reaction planning. ML-based techniques can be particularly relevant for problems involving both computation and experiments. For one, Bayesian inference is a powerful approach to develop models consistent with knowledge from experiments. Second, ML-based methods can also be used to handle problems that are formally intractable using conventional approaches, such as exhaustive characterization of state-to-state information in reactive collisions. Finally, the explicit simulation of reactive networks as they occur in combustion has become possible using machine-learned neural network potentials. This review provides an overview of the questions that can and have been addressed using machine learning techniques, and an outlook discusses challenges in this diverse and stimulating field. It is concluded that ML applied to chemistry problems as practiced and conceived today has the potential to transform the way with which the field approaches problems involving chemical reactions, in both research and academic teaching.},
    author = {Meuwly, Markus},
    date = {2021},
    doi = {10.1021/acs.chemrev.1c00033},
    journaltitle = {Chemical Reviews},
    publisher = {{ACS Publications}},
    title = {Machine {{Learning}} for {{Chemical Reactions}}},
    url = {https://pubs.acs.org/doi/abs/10.1021/acs.chemrev.1c00033}
}

@article{moqadamMembraneModelsMolecular2021,
    abstract = {Peripheral membrane proteins (PMPs) bind temporarily to the surface of biological membranes. They also exist in a soluble form and their tertiary structure is often known. Yet, their membrane-bound form and their interfacial-binding site with membrane lipids remain difficult to observe directly. Their binding and unbinding mechanism, the conformational changes of the PMPs and their influence on the membrane structure are notoriously challenging to study experimentally. Molecular dynamics simulations are particularly useful to fill some knowledge-gaps and provide hypothesis that can be experimentally challenged to further our understanding of PMP-membrane recognition. Because of the time-scales of PMP-membrane binding events and the computational costs associated with molecular dynamics simulations, membrane models at different levels of resolution are used and often combined in multiscale simulation strategies. We here review membrane models belonging to three classes: atomistic, coarse-grained and implicit. Differences between models are rooted in the underlying theories and the reference data they are parameterized against. The choice of membrane model should therefore not only be guided by its computational efficiency. The range of applications of each model is discussed and illustrated using examples from the literature. [GRAPHICS] .},
    annotation = {WOS:000669104100001},
    author = {Moqadam, Mahmoud and Tubiana, Thibault and Moutoussamy, Emmanuel E. and Reuter, Nathalie},
    date = {2021-01-01},
    doi = {10.1080/23746149.2021.1932589},
    issn = {2374-6149},
    journaltitle = {Advances in Physics-X},
    langid = {english},
    location = {{Abingdon}},
    number = {1},
    pages = {1932589},
    publisher = {{Taylor \& Francis Ltd}},
    shortjournal = {Adv. Phys.-X},
    title = {Membrane Models for Molecular Simulations of Peripheral Membrane Proteins},
    url = {https://www.webofscience.com/wos/alldb/summary/44a3404b-9775-4cfb-8e8f-3f1fab40830f-035e7f8d/date-descending/2},
    urldate = {2021-08-06},
    volume = {6}
}

@article{musilPhysicsinspiredStructuralRepresentations2021,
    abstract = {The first step in the construction of a regression model or a data-driven analysis, aiming to predict or elucidate the relationship between the atomic-scale structure of matter and its properties, involves transforming the Cartesian coordinates of the atoms into a suitable representation. The development of atomic-scale representations has played, and continues to play, a central role in the success of machine-learning methods for chemistry and materials science. This review summarizes the current understanding of the nature and characteristics of the most commonly used structural and chemical descriptions of atomistic structures, highlighting the deep underlying connections between different frameworks and the ideas that lead to computationally efficient and universally applicable models. It emphasizes the link between properties, structures, their physical chemistry, and their mathematical description, provides examples of recent applications to a diverse set of chemical and materials science problems, and outlines the open questions and the most promising research directions in the field.},
    author = {Musil, Felix and Grisafi, Andrea and Bartók, Albert P. and Ortner, Christoph and Csányi, Gábor and Ceriotti, Michele},
    date = {2021},
    doi = {10.1021/acs.chemrev.1c00021},
    journaltitle = {Chemical Reviews},
    publisher = {{ACS Publications}},
    title = {Physics-Inspired Structural Representations for Molecules and Materials},
    url = {https://pubs.acs.org/doi/abs/10.1021/acs.chemrev.1c00021}
}

@article{rousseauTheoreticalInsightsSurface2020,
    abstract = {Redox-active oxides find use in many applications, including catalysts, photovoltaic devices, self-cleaning glasses, chemical sensors and electronic components. Their utility derives from their unique ability to access multiple metal-charge states within a finite energy window. However, this property also confounds our ability to study reducible oxides, because it leads to structural, compositional and electronic complexities that elude simplistic models of materials structure and function. Oxygen vacancies play a critical role in shaping the functional properties of such oxides; most notably, they lead to mobile-charge imbalances that impact surface processes at substantial distances from the originating defect. Atomistic simulations are inherently equipped to illuminate these phenomena at a fundamental level; however, reducible oxides pose great challenges, owing to the high level of electron correlation needed to correctly describe them. Understanding how defects form, couple, propagate, agglomerate or repel each other and influence the surface properties of reducible oxides is only now coming into the grasp of modern theory and simulation capabilities. This knowledge is also key to discovering and controlling emergent materials properties with tunable multifunctionalities at the nanometre scale and beyond. Reducible oxides are tunable, multifunctional materials used in many applications, particularly in catalysis; their attractive properties arise from their interacting charge carriers, complex electronic structure and propensity to form mobile defects. This Review surveys theoretical methods to model and understand reducible oxides, using TiO2 as a prototypical example.},
    annotation = {WOS:000535869400002},
    author = {Rousseau, Roger and Glezakou, Vassiliki-Alexandra and Selloni, Annabella},
    date = {2020-06},
    doi = {10.1038/s41578-020-0198-9},
    issn = {2058-8437},
    journaltitle = {Nature Reviews Materials},
    langid = {english},
    location = {{London}},
    number = {6},
    pages = {460--475},
    publisher = {{Nature Publishing Group}},
    shortjournal = {Nat. Rev. Mater.},
    title = {Theoretical Insights into the Surface Physics and Chemistry of Redox-Active Oxides},
    url = {https://www.webofscience.com/wos/alldb/summary/44a3404b-9775-4cfb-8e8f-3f1fab40830f-035e7f8d/date-descending/4},
    urldate = {2021-08-06},
    volume = {5}
}

@article{unkeMachineLearningForce2021,
    abstract = {In recent years, the use of machine learning (ML) in computational chemistry has enabled numerous advances previously out of reach due to the computational complexity of traditional electronic-structure methods. One of the most promising applications is the construction of ML-based force fields (FFs), with the aim to narrow the gap between the accuracy of ab initio methods and the efficiency of classical FFs. The key idea is to learn the statistical relation between chemical structure and potential energy without relying on a preconceived notion of fixed chemical bonds or knowledge about the relevant interactions. Such universal ML approximations are in principle only limited by the quality and quantity of the reference data used to train them. This review gives an overview of applications of ML-FFs and the chemical insights that can be obtained from them. The core concepts underlying ML-FFs are described in detail, and a step-by-step guide for constructing and testing them from scratch is given. The text concludes with a discussion of the challenges that remain to be overcome by the next generation of ML-FFs.},
    author = {Unke, Oliver T. and Chmiela, Stefan and Sauceda, Huziel E. and Gastegger, Michael and Poltavsky, Igor and Schütt, Kristof T. and Tkatchenko, Alexandre and Müller, Klaus-Robert},
    date = {2021},
    doi = {10.1021/acs.chemrev.0c01111},
    journaltitle = {Chemical Reviews},
    publisher = {{ACS Publications}},
    title = {Machine Learning Force Fields},
    url = {https://pubs.acs.org/doi/abs/10.1021/acs.chemrev.0c01111}
}

@article{Weinan2020,
    abstract = {Machine learning is poised as a very powerful tool that can drastically improve our ability to carry out scientific research. However, many issues need to be addressed before this becomes a reality. This article focuses on one particular issue of broad interest: How can we integrate machine learning with physics-based modeling to develop new interpretable and truly reliable physical models? After introducing the general guidelines, we discuss the two most important issues for developing machine learning-based physical models: Imposing physical constraints and obtaining optimal datasets. We also provide a simple and intuitive explanation for the fundamental reasons behind the success of modern machine learning, as well as an introduction to the concurrent machine learning framework needed for integrating machine learning with physics-based modeling. Molecular dynamics and moment closure of kinetic equations are used as examples to illustrate the main issues discussed. We end with a general discussion on where this integration will lead us to, and where the new frontier will be after machine learning is successfully integrated into scientific modeling.},
    author = {Weinan, E and Han, Jiequn and Linfeng, Zhang},
    date = {2020-06},
    title = {Integrating Machine Learning with Physics-Based Modeling},
    url = {https://arxiv.org/abs/2006.02619 http://arxiv.org/abs/2006.02619 https://deepai.org/publication/integrating-machine-learning-with-physics-based-modeling}
}

@article{weinanMachineLearningComputational2020,
    abstract = {Neural network-based machine learning is capable of approximating functions in very high dimension with unprecedented efficiency and accuracy. This has opened up many exciting new possibilities, not just in traditional areas of artificial intelligence, but also in scientific computing and computational science. At the same time, machine learning has also acquired the reputation of being a set of "black box" type of tricks, without fundamental principles. This has been a real obstacle for making further progress in machine learning. In this article, we try to address the following two very important questions: (1) How machine learning has already impacted and will further impact computational mathematics, scientific computing and computational science? (2) How computational mathematics, particularly numerical analysis, can impact machine learning? We describe some of the most important progress that has been made on these issues. Our hope is to put things into a perspective that will help to integrate machine learning with computational mathematics.},
    annotation = {WOS:000592624200002},
    author = {Weinan, E.},
    date = {2020-11},
    doi = {10.4208/cicp.OA-2020-0185},
    issn = {1815-2406},
    journaltitle = {Communications in Computational Physics},
    langid = {english},
    location = {{Wanchai}},
    number = {5},
    pages = {1639--1670},
    publisher = {{Global Science Press}},
    shortjournal = {Commun. Comput. Phys.},
    title = {Machine {{Learning}} and {{Computational Mathematics}}},
    url = {https://www.webofscience.com/wos/alldb/summary/44a3404b-9775-4cfb-8e8f-3f1fab40830f-035e7f8d/date-descending/3},
    urldate = {2021-08-06},
    volume = {28}
}

@article{westermayrMachineLearningElectronically2020,
    abstract = {Electronically excited states of molecules are at the heart of photochemistry, photophysics, as well as photobiology and also play a role in material science. Their theoretical description requires highly accurate quantum chemical calculations, which are computationally expensive. In this review, we focus on not only how machine learning is employed to speed up such excited-state simulations but also how this branch of artificial intelligence can be used to advance this exciting research field in all its aspects. Discussed applications of machine learning for excited states include excited-state dynamics simulations, static calculations of absorption spectra, as well as many others. In order to put these studies into context, we discuss the promises and pitfalls of the involved machine learning techniques. Since the latter are mostly based on quantum chemistry calculations, we also provide a short introduction into excited-state electronic structure methods and approaches for nonadiabatic dynamics simulations and describe tricks and problems when using them in machine learning for excited states of molecules.},
    author = {Westermayr, Julia and Marquetand, Philipp},
    date = {2020},
    doi = {10.1021/acs.chemrev.0c00749},
    journaltitle = {Chemical Reviews},
    publisher = {{ACS Publications}},
    title = {Machine Learning for Electronically Excited States of Molecules},
    url = {https://pubs.acs.org/doi/abs/10.1021/acs.chemrev.0c00749}
}

@article{Zhang2020c,
    abstract = {In recent years, machine learning has emerged as a promising tool for dealing with the difficulty of representing high dimensional functions. This gives us an unprecedented opportunity to revisit theoretical foundations of various scientific fields, develop new schemes, improve existing methodologies, and solve problems that were too complicated for conventional approaches to address. In this dissertation, we identify a list of such problems in the context of multiscale molecular modeling and propose machine learning based strategies to boost simulations with ab initio accuracy to much larger scales than conventional approaches. We consider two representative challenges: 1) how to go from many-electron-ion to atomistic systems, for which the key has been a general and efficient representation of the potential energy surface generated by electronic structure models; 2) how to go from atomistic to coarse-grained systems, for which one is interested in the free energy of the coarse-grained variables as well as the associated dynamical behavior. Our strategies follow two seemingly obvious but non-trivial principles: 1) machine learning based models should respect important physical constraints like symmetry; 2) to build truly reliable models, efficient algorithms are needed to construct a minimal but truly representative training data set. We use these principles to construct the Deep Potential model for the potential energy surface, the Deep Potential Molecular dynamics (DeePMD) which is a new paradigm for performing ab initio molecular dynamics, a concurrent learning scheme (DP-GEN) for generating the data set on the fly, algorithms for constructing the Wannier centers (Deep Wanner) and for efficiently exploring the free energy landscape (Reinforced Dynamics), as well as a machine learning-based coarse grained molecular dynamics model (DeePCG), etc.Applications of these models and algorithms are presented for problems in chemistry, biology, and materials science. Finally, we present our efforts on developing related open-source software packages, which have now been widely used worldwide by experts and practitioners in the molecular simulation community.},
    author = {Zhang, L},
    date = {2020},
    title = {Machine Learning for Multi-Scale Molecular Modeling: Theories, Algorithms, and Applications},
    url = {https://search.proquest.com/openview/58ad7a1fdcc88005de25b81cd4cdc5d8/1?pq-origsite=gscholar&cbl=51922&diss=y}
}

